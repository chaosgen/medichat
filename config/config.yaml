# Model Configuration
model:
  hidden_size: 512
  max_seq_length: 512
  num_attention_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dropout: 0.1

# Training Configuration
training:
  batch_size: 32
  learning_rate: 0.001
  num_epochs: 100
  eval_steps: 1000
  save_steps: 5000

# Data Configuration
data:
  raw_data_path: "data/raw/mle_screening_dataset.csv"
  processed_data_dir: "data/processed"
  train_file: "data/processed/train.csv"
  eval_file: "data/processed/eval.csv"
  test_file: "data/processed/test.csv"
  vocab_file: "data/processed/vocab.txt"
  eval_size: 0.1
  test_size: 0.1

# Model Paths
paths:
  checkpoint_dir: "models/checkpoints"
  latest_checkpoint: "models/checkpoints/checkpoint_epoch_10.pt"

# LLM Configuration
llm:
  model_path: "models/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf"  # Will be downloaded
  n_gpu_layers: 35  # Adjust based on your VRAM
  n_ctx: 2048
  n_batch: 512
  max_tokens: 512
  temperature: 0.7
  top_p: 0.95
  template: |
    System: You are a helpful medical assistant. Provide accurate, professional answers based on the given context. Keep responses clear and concise.
    
    Context: {context}
    
    Human: {question}
    
    Assistant: 
    
# Vector Database Configuration
vector_db:
  collection_name: "medichat_docs"
  chunk_size: 500
  chunk_overlap: 50
  embedding_model: "BAAI/bge-small-en-v1.5"
  top_k: 4

# RAG Configuration
rag:
  rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  rerank_top_k: 2
  max_context_length: 2048

# API Configuration
api:
  host: "0.0.0.0"
  port: 5000
  max_sequence_length: 512  # Increased for LLM context
